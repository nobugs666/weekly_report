## 1.12
### Geopipe内容梳理
**研究背景**： LLM 参数规模指数级增长，单数据中心训练受限于电力供给、GPU 资源碎片化、数据隐私保护等问题，地理分布式多数据中心训练成为必然趋势，但跨数据中心通信延迟高、丢包率高、计算空洞等问题严重制约训练效率。

**核心方案**：提出 GeoPipe 框架，通过 “增强型流水线并行策略 + 无损 RDMA 光传输网络（DC-OTN）+ 遗传启发式参数优化” 的协同设计，解决跨数据中心 LLM 训练的效率瓶颈。

**创新点**：
1.提出增强型流水线并行方案，通过调度提前量实现跨数据中心通信与计算的高效重叠。
2.整合无损 RDMA 光传输网络（DC-OTN），提供确定性延迟和零丢包传输。
3.设计遗传启发式优化算法，自适应调整微批次大小、序列长度和 ΔN，在 HBM 与带宽约束下最小化迭代时间。

**明日计划**：补充分布式训练知识，补充查找文献中其他论文方案。

## 1.13

### 学习分布式训练
#### 显存占用分析:
Model States:模型本身存储参数
- Parameters：模型参数
- Gradients：模型梯度
- Optimizer State:优化器参数
Residual States：训练过程产生
- Activation：激化值
- Temporary Buffers：临时存储
- Unusable Fragmented Memory:碎片化存储空间

#### 三种节约显存技术: 
- 梯度检查点：将模型的任意部分 用 [torch.utils.checkpoint](https://link.zhihu.com/?target=https%3A//pytorch.org/docs/stable/checkpoint.html) 封装。前向传播时激活值不会保存，在反向传播时, 我们会重新计算 部分的 激活值。

- 梯度累加：将一个大的 mini-batch 拆成k个 macro-batch, 每一个 macro-batch 在分别进行前向传播和反向传播运算。当k次反向传播运算完成后再更新参数，这样和直接用 mini-batch 效果是一致的，每一个 macro-batch 计算出来的 loss 值要除以k。

- 混合精度训练：结合使用不同精度的数据类型。

#### 八种Pytorch提供的集合通信：

**(1)** broadcast(tensor, src)

```python
tensor = torch.randn(3, 4)
dist.broadcast(tensor, src=0)
```

**(2)** all_reduce(tensor, op)
```python
tensor = torch.randn(3, 4)
dist.all_reduce(tensor, op=dist.ReduceOp.SUM)
```

**(3)** reduce(tensor, dst, op)
```python
tensor = torch.randn(3, 4)
dist.reduce(tensor, dst=0, op=dist.ReduceOp.MAX)
```

**(4)** all_gather(tensor_list, tensor)
```python
tensor = torch.ones(2) * (dist.get_rank() + 1) 
tensor_list = [torch.zeros(2) for _ in range(dist.get_world_size())]
dist.all_gather(tensor_list, tensor)
```

**(5)** gather(tensor, gather_list, dst)

**(6)** scatter(tensor, scatter_list, src)

**(7)** reduce_scatter(output, input_list, op)

**(8)** all_to_all(output_tensor_list, input_tensor_list)

集合通信与点到点通信区别：多个与两个

**明日计划**：学习Gpipe论文

## 1.14
### 学习论文[Gpipe](https://arxiv.org/abs/1811.06965)

研究背景：随着神经网络模型容量提升，单加速器内存限制成为瓶颈，现有模型并行方案多依赖特定架构或任务，缺乏通用性和高效性，亟需灵活适配多任务的模型并行框架。

核心方案：提出 GPipe **流水线并行**库，通过 “模型分层分区 + 微批次拆分 + 激活重计算” 的组合设计，实现任意序列型网络的高效扩展，支持在多加速器上训练超大规模模型。

创新点：
- 模型分区策略：将网络按层划分为 K 个连续单元（cell），每个单元部署在独立加速器，自动平衡各单元计算成本以提升流水线效率；

- 微批次流水线算法：即梯度检查点。将迷你批次拆分为 M 个微批次，通过流水线调度使不同加速器同时处理不同微批次，梯度在所有微批次完成后同步更新，保证训练一致性；

- 激活重计算优化：反向传播时仅保留分区边界的输出激活，激活通过重计算生成，走了两次forward运算；

- 通信优化：仅在分区边界传输激活张量，开销低。

**明日计划**：学习多种并行训练方式

## 1.15
 ### 并行方法学习
 **数据并行DP:**

沿着 batch 的维度进行拆分。如果我们有 个 GPU 设备, 那么我们可以将一个 mini-batch 拆分成 个 macro-batch, 每一个 GPU 设备处理一个 macro-batch, 然后将不同 GPU 设备计算出来的梯度取平均

**ZeRo数据并行**：

ZeRO-1：对优化器状态进行划分

ZeRO-2：对优化器状态和梯度进行划分

ZeRO-3：对优化器状态、梯度和参数进行划分（最节省显存）

**流水线并行PP**:对模型最简单的切分方式是按照层切分， 不同层之间按照顺序执行。[GPipe](https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1811.06965) 中提出了 **流水线并行** (pipeline parallelism), 使用 **数据并行** 的策略增加并行计算的程度。

**张量并行TP:**张量并行是将模型的张量（如权重矩阵）按维度切分到不同的GPU上运行,PP是垂直分割，张量为横向分割。

**混合并行**：3D并行

DP,PP,TP三者优缺点

明日计划：学习论文Megatron LM 

## 1.16

### 1.复习transformer架构

### 2.学习论文[Megatron LM](https://arxiv.org/abs/1909.08053)

**研究背景**：随着自然语言处理领域的发展，大规模 Transformer 模型的训练成为提升任务性能的关键，但单 GPU 内存限制模型。现有优化算法（如 Adam）需额外存储动量等状态，传统模型并行方案（如 GPipe、Mesh-TensorFlow）依赖定制编译器或框架重构，落地成本高且灵活性不足，无法高效适配原生 PyTorch 生态；BERT 类模型在参数规模扩大时会出现性能退化问题。

**核心方案**：提出基于**层内模型并行（intra-layer model parallelism）** 的高效训练框架,专门针对基于Transformer的语言模型（如GPT-2和BERT）设计。MLP,自注意力并行，嵌入层并行，优化通信。

**创新点：**

- 仅需在现有PyTorch Transformer实现中插入少量通信原语

- 层内并行与管道并行正交

- 通过调整层归一化与残差连接的顺序优化BERT架构

------

**本周总结**：学习分布式训练概念，并行方式，Gpipe，Megatron系统，但并未学习代码实现，理解不够深入，几种并行方式中通信操作未涉及。

**下周计划**：结合代码理解Megatron LM,继续阅读相关论文如PipeDream,Zero等，学习并行计算的代码操作。
